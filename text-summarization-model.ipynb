{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "* **Dhruvi Khandelwal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Import Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.0rc0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.16.0-rc0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.0rc0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (3.0.5)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.0-rc0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.0-rc0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.0-rc0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.0-rc0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.0-rc0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.0-rc0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.0-rc0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dhruvi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.0-rc0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "from tensorflow.python.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Import Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet sets up the environment to load a CSV file containing news summaries into a pandas DataFrame and displays the first few rows to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##Specifying the Path to the Dataset\n",
    "data_path = '../input/news-summary/news_summary_more.csv'\n",
    "\n",
    "#Loading the Dataset\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "#Displaying the First Few Rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Removing Duplicates\n",
    "data.drop_duplicates(subset=['headlines'],inplace=True)\n",
    "\n",
    "#Resetting the Index\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The preprocess function performs several steps to clean and standardize text data:\n",
    "* Converts text to lowercase.\n",
    "* Expands contractions.\n",
    "* Removes stop words.\n",
    "* Removes possessive 's and periods.\n",
    "* Removes text within parentheses.\n",
    "* Replaces non-alphanumeric characters (except periods and spaces) with spaces.\n",
    "* Ensures spaces follow periods.\n",
    "* Replaces multiple spaces with a single space.ngle space.ngle space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Loading Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#Defining the preprocess Function\n",
    "def preprocess(text):\n",
    "    \n",
    "    #Converting Text to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Expanding Contractions\n",
    "    text = ' '.join([contractions.fix(word) for word in text.split(\" \")])    \n",
    "    \n",
    "    #Removing Stop Words\n",
    "    tokens = [w for w in text.split() if not w in stop_words]\n",
    "    \n",
    "    #Additional Text Cleaning\n",
    "    text = \" \".join(tokens)\n",
    "    text = text.replace(\"'s\",'')\n",
    "    text = text.replace(\".\",'')\n",
    "    text = re.sub(r'\\(.*\\)','',text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]',' ',text)\n",
    "    text = re.sub(r'\\.','. ',text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    #Returning the Cleaned Text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code preprocesses the text data in both the 'headlines' and 'text' columns, adds special tokens to the 'headlines', and prints the first two examples.\n",
    "* **Preprocessing Columns**: Both the 'headlines' and 'text' columns are preprocessed to clean and standardize the text using the preprocess function defined earlier. This ensures that the text is in a consistent format, which is crucial for tasks like text summarization or machine learning.\n",
    "* **Adding Tokens**: The start (_START_) and end (_END_) tokens are added to the 'headlines' to indicate the beginning and end of the summary. This is often done in sequence-to-sequence models, such as those used in text generation or summarization, to help the model understand where the summary starts and ends.\n",
    "* **Printing Examples**: Printing the first two processed entries allows for a quick inspection to ensure that the preprocessing steps have been applied correctly and to see examples of the formatted text and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing the 'headlines' Column\n",
    "data['headlines'] = data['headlines'].apply(preprocess)\n",
    "\n",
    "#Preprocessing the 'text' Column\n",
    "data['text'] = data['text'].apply(preprocess)\n",
    "\n",
    "#Adding Start and End Tokens to 'headlines'\n",
    "data['headlines'] = data['headlines'].apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "#Printing Processed Examples\n",
    "for i in range(2):\n",
    "    print('Summary:', data['headlines'][i],'Text:', data['text'][i], sep='\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The provided lines of code generate two lists: headlines_length and text_length. Each element in these lists represents the number of words in a corresponding entry in the 'headlines' and 'text' columns, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Calculating the Lengths of 'headlines'\n",
    "headlines_length = [len(x.split()) for x in data.headlines]\n",
    "\n",
    "#Calculating the Lengths of 'text'\n",
    "text_length = [len(x.split()) for x in data.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code below creates a figure with two side-by-side histograms to visualize the distribution of word counts in the 'headlines' and 'text' columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Creating a Figure and Subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "\n",
    "#Plotting Histograms\n",
    "ax1.hist(headlines_length, bins = 20)\n",
    "ax2.hist(text_length, bins = 20)\n",
    "\n",
    "#Setting Titles for Subplots\n",
    "ax1.title.set_text(\"Words in Headlines\")\n",
    "ax2.title.set_text(\"Words in Text\")\n",
    "\n",
    "#Displaying the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Embedding Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code below sets up for using GloVe embeddings by specifying the embedding size (300 dimensions) and opening the GloVe embeddings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Setting the Embedding Size\n",
    "glove_size = 300\n",
    "\n",
    "#Opening the GloVe Embeddings File\n",
    "f = open('../input/glove42b300dtxt/glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet below reads the pre-trained GloVe word embeddings from a file and stores them in a dictionary called embeddings_index. Each word in the dictionary is associated with its corresponding 300-dimensional word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Creating an Empty Dictionary for Embeddings\n",
    "embeddings_index = dict()\n",
    "\n",
    "#Iterating Over Each Line in the File\n",
    "for line in f:\n",
    "    \n",
    "    #Processing Each Line\n",
    "    values = line.split()\n",
    "    \n",
    "    #Storing the Word and Its Embedding in the Dictionary\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "    \n",
    "#Closing the File\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet processes the text data to:\n",
    "* Collect all words and unique words in the corpus.\n",
    "* Compare the unique words with the GloVe vocabulary to determine how many words have corresponding GloVe vectors.\n",
    "* Create a dictionary of word vectors for the words in the corpus that have corresponding GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Collecting All Words in the Corpus\n",
    "words_source_train = []\n",
    "for i in data['text'] :\n",
    "  words_source_train.extend(i.split(' '))\n",
    "\n",
    "#Printing the Total Number of Words in the Corpus\n",
    "print(\"all the words in the corpus\", len(words_source_train))\n",
    "\n",
    "#Identifying Unique Words in the Corpus\n",
    "words_source_train = set(words_source_train)\n",
    "print(\"the unique words in the corpus\", len(words_source_train))\n",
    "\n",
    "#Finding Intersection with GloVe Vocabulary\n",
    "inter_words = set(embeddings_index.keys()).intersection(words_source_train)\n",
    "print(\"The number of words that are present in both glove vectors and our corpus are {} which \\\n",
    "is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))/len(words_source_train))\n",
    "*100)))\n",
    "\n",
    "#Creating a Dictionary of Word Vectors for the Corpus\n",
    "words_corpus_source_train = {}\n",
    "words_glove = set(embeddings_index.keys())\n",
    "for i in words_source_train:\n",
    "  if i in words_glove:\n",
    "    words_corpus_source_train[i] = embeddings_index[i]\n",
    "print(\"word 2 vec length\", len(words_corpus_source_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For the code cell below:\n",
    "* **Purpose**: This line of code identifies and prints a sample of words from the corpus that do not have corresponding GloVe vectors. This can help us understand which words in our corpus are missing from the pre-trained embeddings and may need to be handled separately (e.g., by using random vectors, learning embeddings from scratch, or ignoring them).\n",
    "* **Use Case**: This is useful for diagnosing coverage issues with the embeddings and understanding the extent to which our corpus differs from the vocabulary covered by the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Finding and Printing Words Not in GloVe\n",
    "print(list(words_source_train - inter_words)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code defines a function to count words in each text entry of a DataFrame that are not present in a predefined set of words with GloVe vectors. This count is then added as a new column in the DataFrame, allowing for analysis of the extent to which words in the text are missing from the GloVe embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Defining the Function num\n",
    "def num(text):\n",
    "  words = [w for w in text.split() if not w in inter_words]\n",
    "  return len(words)\n",
    "\n",
    "#Applying the Function to the DataFrame\n",
    "data['unique_words'] = data['text'].apply(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The value_counts() method on the 'unique_words' column provides a count of how many entries have each unique number of words that are missing from the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['unique_words'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet filters the DataFrame data to retain only those rows where the number of unique words not present in GloVe embeddings is less than 4. It then resets the index of the DataFrame to ensure that it is sequential and free from gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Filtering Rows\n",
    "data = data[data['unique_words'] < 4]\n",
    "\n",
    "#Resetting the Index\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Train Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code performs two sequential splits on the dataset to create three subsets:\n",
    "* **Training Set (X_train, y_train)**: 80% of the data used to train the model.\n",
    "* **Validation Set (X_val, y_val)**: 10% of the data used to tune model parameters and avoid overfitting.\n",
    "* **Test Set (X_test, y_test)**: 10% of the data used to evaluate the final model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#First Split: Train and Validation Sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['text'], data['headlines'], test_size = 0.2, random_state = 20)\n",
    "\n",
    "#Second Split: Validation and Test Sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size = 0.5, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The lines of code in the cell below calculate and store the lengths of the longest text and headline in the dataset. The maximum lengths are essential for preparing the data for machine learning models, ensuring that all sequences are of a consistent size by padding or truncating them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length_x = max(text_length)\n",
    "max_length_y = max(headlines_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The process in the cell below transforms raw text data into a format that can be used by machine learning models, ensuring that all input sequences have a consistent size and are represented as integer indices.\n",
    "* **Tokenizer Initialization and Fitting**: The Tokenizer is used to create a vocabulary from all the texts and headlines, mapping each word to a unique integer.\n",
    "* **Vocabulary Size**: The vocabulary size is determined and adjusted to include an index for padding.\n",
    "* **Text to Sequences**: Texts are converted to sequences of integers based on the vocabulary.\n",
    "* **Padding Sequences**: Sequences are padded to ensure they all have the same length, making them suitable for input into neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Creating and Fitting the Tokenizer\n",
    "x_t = Tokenizer()\n",
    "x_t.fit_on_texts(data['text'] + data['headlines'])\n",
    "\n",
    "#Getting Vocabulary Size\n",
    "x_vocab_size = len(x_t.word_index) + 1\n",
    "\n",
    "#Encoding Texts into Sequences\n",
    "encoded_xtrain = x_t.texts_to_sequences(X_train)\n",
    "encoded_xval = x_t.texts_to_sequences(X_val)\n",
    "encoded_xtest = x_t.texts_to_sequences(X_test)\n",
    "\n",
    "#Padding Sequences\n",
    "padded_xtrain = pad_sequences(encoded_xtrain, maxlen=max_length_x, padding='post')\n",
    "padded_xval = pad_sequences(encoded_xval, maxlen=max_length_x, padding='post')\n",
    "padded_xtest = pad_sequences(encoded_xtest, maxlen=max_length_x, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Creating and Fitting the Tokenizer\n",
    "y_t = Tokenizer()\n",
    "y_t.fit_on_texts(data['headlines'])\n",
    "\n",
    "#Getting Vocabulary Size\n",
    "y_vocab_size = len(y_t.word_index) + 1\n",
    "\n",
    "#Encoding Texts into Sequences\n",
    "encoded_ytrain = y_t.texts_to_sequences(y_train)\n",
    "encoded_yval = y_t.texts_to_sequences(y_val)\n",
    "encoded_ytest = y_t.texts_to_sequences(y_test)\n",
    "\n",
    "#Padding Sequences\n",
    "padded_ytrain = pad_sequences(encoded_ytrain, maxlen=max_length_y, padding='post')\n",
    "padded_yval = pad_sequences(encoded_yval, maxlen=max_length_y, padding='post')\n",
    "padded_ytest = pad_sequences(encoded_ytest, maxlen=max_length_y, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The embedding matrix will be used in our model to provide pre-trained word vectors as input, improving the model's ability to understand and process text based on GloVe's semantic embeddings.\n",
    "* **Loaded Word Vectors**: The number of word vectors loaded from GloVe is printed to verify successful loading.\n",
    "* **Embedding Matrix Initialization**: A matrix is created to hold the word embeddings for the vocabulary. This matrix will be used in the model to represent words with their GloVe vectors.\n",
    "* **Matrix Population**: The embedding matrix is populated with GloVe vectors for words that exist in the vocabulary. Words not found in GloVe remain as zeros, which are usually handled by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Printing the Count of Loaded Vectors\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Initializing the Embedding Matrix\n",
    "embedding_matrix = np.zeros((x_vocab_size, glove_size))\n",
    "\n",
    "#Populating the Embedding Matrix\n",
    "for word, i in x_t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "The AttentionLayer implements a custom attention mechanism using a specific architecture:\n",
    "* **Weights**: W_a, U_a, and V_a are used for computing attention scores.\n",
    "* **Attention Computation**: The energy_step function calculates attention scores, and the context_step function computes the context vectors.\n",
    "* **Initial States**: Dummy initial states are created for the RNN computations.\n",
    "* **RNN Operations**: The attention mechanism is applied using K.rnn to calculate attention scores and context vectors.\n",
    "* **Output Shapes**: The output shapes are defined for the context vectors and attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "          \n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  \n",
    "            \n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            \n",
    "            fake_state = K.zeros_like(inputs)  \n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  \n",
    "            fake_state = K.expand_dims(fake_state)  \n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  \n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  \n",
    "\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code constructs a sequence-to-sequence model with an attention mechanism:\n",
    "* **Encoder**: Uses a stack of LSTM layers to process the input sequence.\n",
    "* **Decoder**: Uses an LSTM layer initialized with the encoder's final states to generate output sequences.\n",
    "* **Attention Mechanism**: Enhances the decoder by focusing on relevant parts of the encoder's output.\n",
    "* **Final Layer**: Applies a dense layer to predict the next token in the sequence at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "latent_dim=500\n",
    "\n",
    "K.clear_session() \n",
    "\n",
    "encoder_inputs = Input(shape=(max_length_x,)) \n",
    "enc_emb = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False)(encoder_inputs) \n",
    "\n",
    "#LSTM \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The setup ensures efficient training by saving the best model and preventing overfitting through early stopping.\n",
    "* **Compiling**: Configures the model for training with the Adam optimizer and sparse categorical crossentropy loss.\n",
    "* **Callbacks**: Includes model checkpointing to save the best model and early stopping to halt training if validation loss does not improve.\n",
    "* **Training**: Trains the model for 10 epochs with a batch size of 512, using both training and validation data, and applying the specified callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Model Compilation\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#Model Checkpoint\n",
    "checkpoint_filepath = './model.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,\n",
    "                                                               monitor='val_loss',\n",
    "                                                               mode='min',\n",
    "                                                               save_best_only=True, \n",
    "                                                               save_freq = \"epoch\")\n",
    "\n",
    "#Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "#Model Training\n",
    "history=model.fit([padded_xtrain,padded_ytrain[:,:-1]], padded_ytrain.reshape(padded_ytrain.shape[0],padded_ytrain.shape[1], 1)[:,1:] ,\n",
    "                  epochs=10,\n",
    "                  batch_size=512, \n",
    "                  validation_data=([padded_xval,padded_yval[:,:-1]], padded_yval.reshape(padded_yval.shape[0],padded_yval.shape[1], 1)[:,1:]), \n",
    "                  callbacks=[es, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_best_model_file(directory, pattern=\"model.*.h5\"):\n",
    "    files = os.listdir(directory)\n",
    "    best_loss = float('inf')\n",
    "    best_model_file = None\n",
    "    \n",
    "    for file in files:\n",
    "        match = re.search(r'(\\d+\\.\\d+)\\.h5$', file)\n",
    "        if match:\n",
    "            val_loss = float(match.group(1))\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model_file = file\n",
    "    \n",
    "    return os.path.join(directory, best_model_file) if best_model_file else None\n",
    "\n",
    "# Load the best model weights\n",
    "best_model_filepath = find_best_model_file('./')\n",
    "if best_model_filepath:\n",
    "    model.load_weights(best_model_filepath)\n",
    "    print(f\"Loaded weights from {best_model_filepath}\")\n",
    "else:\n",
    "    print(\"No model file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss = model.evaluate([padded_xval, padded_yval[:, :-1]], padded_yval.reshape(padded_yval.shape[0], padded_yval.shape[1], 1)[:, 1:])\n",
    "print(f\"Validation loss after loading best weights: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The snippet of code is used to load a previously saved model's weights and recompile it. \n",
    "* **Reconstructing the Model**: Recreates the model architecture to match the one used during training. This step is crucial because the model needs to have the same structure to correctly load the weights.\n",
    "* **Loading Weights**: Loads the weights from a specific file into the reconstructed model. This allows us to use the trained model’s parameters without retraining.\n",
    "* **Recompiling**: Sets up the model with the same optimizer and loss function used during training, preparing it for evaluation or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Reconstructing the Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Loading Weights\n",
    "model.load_weights(\"./model.10-3.21.h5\")\n",
    "\n",
    "#Recompiling the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This code creates a plot showing how the training and validation loss change over epochs. This visualization helps in understanding the model's performance during training:\n",
    "* **Training Loss (train)**: Indicates how well the model is fitting the training data.\n",
    "* **Validation Loss (test)**: Indicates how well the model is generalizing to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "\n",
    "#Plotting Training and Validation Loss\n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The mappings below are crucial for tasks such as converting model predictions back into readable words or for analyzing and interpreting the model’s output.\n",
    "* **reverse_target_word_index**: Maps indices to words for the target vocabulary.\n",
    "* **reverse_source_word_index**: Maps indices to words for the source vocabulary.\n",
    "* **target_word_index**: Maps words to indices for the target vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Reverse Target Word Index\n",
    "reverse_target_word_index = y_t.index_word \n",
    "\n",
    "#Reverse Source Word Index\n",
    "reverse_source_word_index = x_t.index_word \n",
    "\n",
    "#Target Word Index\n",
    "target_word_index = y_t.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "These models are used together for generating sequences during inference. The encoder processes the input sequence, and the decoder generates the output sequence step-by-step, using the attention mechanism to focus on relevant parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Encoder Model\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#Decoder Model\n",
    "#Defining Input Layers for Decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_length_x,latent_dim))\n",
    "\n",
    "#Decoder Embedding and LSTM\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "#Concatenate and Dense Layer\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#Attention Mechanism\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "\n",
    "#Concatenate and Dense Layer\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "#Define Decoder Model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The function below is typically used for generating predictions\n",
    "* **Encoding**: The input sequence is processed by the encoder to obtain the encoder outputs and states.\n",
    "* **Decoding**: The decoder generates the sequence one token at a time using the attention mechanism and updated states.\n",
    "* **Termination**: The loop continues until an end token is generated or the sentence reaches the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Function Definition\n",
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    #Preprocessing Input Sequence\n",
    "    input_seq= input_seq.reshape(1,max_length_x)\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Initialization\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    #Decoding Loop\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "  \n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    " \n",
    "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_length_y-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    #Return Decoded Sentence\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Both functions below are useful for interpreting model outputs and inputs by translating numerical representations back into human-readable form.\n",
    "* **seq2summary**: Converts a target sequence from indices to a readable summary text, excluding special tokens like start and end tokens.\n",
    "* **seq2text**: Converts a source sequence from indices to a readable text, excluding padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Function: seq2summary\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "#Function: seq2text\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet below evaluates the performance of the sequence-to-sequence model by comparing the original summaries with the predicted summaries for a sample of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(\"Review:\",seq2text(padded_xtest[i]))\n",
    "  print(\"Original summary:\",seq2summary(padded_ytest[i]))\n",
    "  print(\"Predicted summary:\",decode_sequence(padded_xtest[i]))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The **BLEU_Score** function is designed to evaluate the quality of machine-generated summaries by calculating the **BLEU** (Bilingual Evaluation Understudy) score. BLEU is a metric for evaluating the quality of text generated by a model compared to reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Function Definition\n",
    "def BLEU_Score(y_test, y_pred):\n",
    "    \n",
    "    #Convert Target Summary (y_test) to Readable Text:\n",
    "    references = [[seq2summary(y_test).split(\" \")]]\n",
    "    \n",
    "    #Convert Predicted Summary (y_pred) to Readable Text:\n",
    "    candidates = [decode_sequence(y_pred.reshape(1,max_length_x)).split(\" \")]\n",
    "    \n",
    "    #Compute BLEU Score:\n",
    "    return corpus_bleu(references, candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The code snippet calculates and prints the average BLEU score for the first 500 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "#Initialize an Empty List:\n",
    "scores=[]\n",
    "\n",
    "#Loop Through First 500 Test Samples:\n",
    "for i in range(0,500):\n",
    "    \n",
    "    #Calculate BLEU Score for Each Sample:\n",
    "    scores.append(BLEU_Score(padded_ytest[i],padded_xtest[i]))\n",
    "    \n",
    "#Calculate and Print the Average BLEU Score:\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The Universal Sentence Encoder is designed to provide embeddings for sentences\n",
    "* **Purpose**: To load a pre-trained Universal Sentence Encoder model from TensorFlow Hub, which can be used to convert sentences into vector embeddings for various natural language processing tasks.\n",
    "* **Inputs**: The URL of the pre-trained model.\n",
    "* **Outputs**: A confirmation message that the model has been successfully loaded and is ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "\n",
    "#Define the URL for the Model:\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "#Load the Model from TensorFlow Hub:\n",
    "sentence_encoder = hub.load(module_url)\n",
    "\n",
    "#Print Confirmation Message:\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "The cosine similarity score provides a way to evaluate how closely the generated summaries match the actual summaries in terms of their semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Function Definition\n",
    "def cosine_similarity(padded_xval, padded_yval):\n",
    "    \n",
    "    #Initialize an Empty List for Scores:\n",
    "    scores = []\n",
    "    for i in range(len(padded_xval)):\n",
    "        \n",
    "        #Convert Token Sequences to Text:\n",
    "        str1 = seq2summary(padded_yval[i])\n",
    "        str2 = decode_sequence(padded_xval[i])\n",
    "        \n",
    "        #Get Sentence Embeddings:\n",
    "        embeddings = sentence_encoder([str1, str2]).numpy()\n",
    "        \n",
    "        #Compute Cosine Similarity:\n",
    "        result = 1 - spatial.distance.cosine(embeddings[0], embeddings[1])\n",
    "        \n",
    "        #Append the Similarity Score:\n",
    "        scores.append(result)\n",
    "        \n",
    "    #Return the List of Scores:\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Compute Cosine Similarity Scores:\n",
    "scores = cosine_similarity(padded_xtest[:500],padded_ytest[:500] )\n",
    "\n",
    "#Calculate the Mean Cosine Similarity Score:\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
